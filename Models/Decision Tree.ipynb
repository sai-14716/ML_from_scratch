{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46267042-4c82-4da9-9156-ac59ec20011f",
   "metadata": {},
   "source": [
    "## Importing the required libraries \n",
    "**Note: Add them into the requirements.txt to install them before using this model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6b3897a-7158-4565-8020-cd417a60223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cf4cfb0-0f1b-4758-bb90-b35579cab193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Age Sex      BP Cholesterol  Na_to_K   Drug\n",
      "0     23   F    HIGH        HIGH   25.355  drugY\n",
      "1     47   M     LOW        HIGH   13.093  drugC\n",
      "2     47   M     LOW        HIGH   10.114  drugC\n",
      "3     28   F  NORMAL        HIGH    7.798  drugX\n",
      "4     61   F     LOW        HIGH   18.043  drugY\n",
      "..   ...  ..     ...         ...      ...    ...\n",
      "195   56   F     LOW        HIGH   11.567  drugC\n",
      "196   16   M     LOW        HIGH   12.006  drugC\n",
      "197   52   M  NORMAL        HIGH    9.894  drugX\n",
      "198   23   M  NORMAL      NORMAL   14.020  drugX\n",
      "199   40   F     LOW      NORMAL   11.349  drugX\n",
      "\n",
      "[200 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('drug200.csv')\n",
    "print(df)\n",
    "\n",
    "data = df.to_numpy()\n",
    "# print(data)\n",
    "X = data[:150, :-1]\n",
    "# print(X)\n",
    "y = data[:150, -1]\n",
    "# print(y)\n",
    "X_type = np.array(([1, 0, 0, 0, 1]))\n",
    "y_type = np.array(([0]))\n",
    "# print(X_type, y_type)\n",
    "X_test = data[150:, :-1]\n",
    "y_test = data[150:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5998be6c-0064-429c-b755-172c6fded80c",
   "metadata": {},
   "source": [
    "# IMPLEMENTING THE DESICION TREES\n",
    "## Features, Targets types are Categorical, Continuous\n",
    "### We will pre-process data with the Categorical or Continuous in the 1st column\n",
    "**Pre-processing:**\n",
    "1. If User has a pre-defined order of the features\n",
    "2. Ask the user for the *max_feautures* (The number of feautures considered for the splitting) for the continution for **Random Forests**\n",
    "3. Ask the user for the *min_samples*\n",
    "4. Ask the user for the *max_depth*\n",
    "\n",
    "**Note: If No order is taken then random order of the random features will be selected**\n",
    "\n",
    "## Important Notes:\n",
    "### The following are the possible feature, label values cat -> Categorical, con -> Continuous\n",
    "1. The Threshold is all uniques values for cat and we will divide the continuous based on **avg_childs**\n",
    "2. The GAIN function is **entropy** in the case of the **cat** and the gain is **variance** in the case of **con**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08ab1d54-c4f5-48df-9262-4443e515fe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision_Tree:\n",
    "    def __init__(self, max_features = np.inf, max_depth = None, min_samples = 2, avg_childs = 2, *, order = None):\n",
    "        self.max_features = max_features\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples = min_samples\n",
    "        self.order = order\n",
    "        self.nodes = 0\n",
    "        #for continuous cases\n",
    "        self.avg_childs = avg_childs\n",
    "        \n",
    "    def build_tree(self, parent, X, y, X_type, y_type):\n",
    "\n",
    "        self.max_features = X.shape[1] if self.max_features is None else self.max_features\n",
    "        self.max_depth = np.inf if self.max_depth is None else self.max_depth\n",
    "        \n",
    "        #BFS method -- You can easily incorparate the max_depth if you have bfs knowledge\n",
    "        q = deque()\n",
    "        depth = 0\n",
    "        feature_map = np.array(([i for i in range(X.shape[1])]))\n",
    "        q.append([parent, X, y, X_type, y_type, depth, feature_map])\n",
    "        \n",
    "        while q:            \n",
    "            child = q.popleft()\n",
    "            #check exit condition\n",
    "            features, status = self._check_exit_condition(child)\n",
    "            if status == \"leaf\":\n",
    "                continue\n",
    "            #find the best split for this node, update the node and q\n",
    "            feature, threshold = self._select_best_feature(features, child[1] ,child[2], child[3], child[4])\n",
    "        \n",
    "            child[0].feature = child[6][feature]\n",
    "            childs = []\n",
    "            q, childs = self._update_q_and_childs(child, feature, threshold, q, childs)\n",
    "            \n",
    "            child[0].feature_type = child[3][feature]\n",
    "            child[0].dividers = threshold\n",
    "            child[0].childs = childs\n",
    "            \n",
    "            self.nodes = self.nodes + 1\n",
    "            self.avg_childs = self.avg_childs * self.nodes + len(childs)\n",
    "            self.avg_childs = self.avg_childs / self.nodes\n",
    "\n",
    "        return \n",
    "        \n",
    "    def _check_exit_condition(self, child):\n",
    "        \n",
    "        status = \"non_leaf\"\n",
    "       \n",
    "        n_features = child[1].shape[1]\n",
    "        features = None\n",
    "\n",
    "        self.max_features = min(n_features, self.max_features)\n",
    "        \n",
    "        if(n_features < 2 or child[1].shape[0] < self.min_samples or child[5] > self.max_depth):\n",
    "            child_value = None\n",
    "            \n",
    "            if child[4] == 0:\n",
    "                #categorical\n",
    "                values, counts = np.unique(child[2], return_counts = True)\n",
    "                child_value = dict(zip(values, counts))\n",
    "            else:\n",
    "                #continuous\n",
    "                child_value = child[2].mean(axis = 0)\n",
    "                \n",
    "            child[0].value = child_value\n",
    "           \n",
    "            status = \"leaf\"\n",
    "            \n",
    "            return n_features, status\n",
    "            \n",
    "            \n",
    "        # select the features to build the tree\n",
    "        if(self.order is not None):\n",
    "            features = [self.order[i] for i in range(self.max_features)]\n",
    "        else:\n",
    "            features = np.random.choice(np.arange(0, n_features), size = self.max_features, replace = False)\n",
    "\n",
    "\n",
    "        return features, status\n",
    "        \n",
    "    def _update_q_and_childs(self, child, feature, threshold, q, childs):\n",
    "        \n",
    "        prev_idx = -np.inf #used in continuous case\n",
    "\n",
    "        feature_map = np.delete(child[6], feature)\n",
    "        \n",
    "        for idx in threshold:\n",
    "            mask = None\n",
    "            if child[3][feature] == 0:\n",
    "                #categorical\n",
    "                mask = child[1][:, feature] == idx\n",
    "            else:\n",
    "                #continuos\n",
    "                mask = (child[1][:, feature] > prev_idx) & (child[1][:, feature] < idx)\n",
    "                prev_idx = idx\n",
    "            \n",
    "            X_child = np.delete(child[1][mask, :], feature, axis = 1)\n",
    "            y_child = child[2][mask]\n",
    "            X_type_child = np.delete(child[3], feature)\n",
    "            y_type_child = child[4]\n",
    "            \n",
    "            new_child = Node()\n",
    "            \n",
    "            if(len(y_child) > 0):\n",
    "                \n",
    "                childs.append(new_child)\n",
    "                q.append([new_child, X_child, y_child, X_type_child, y_type_child, child[5] + 1, feature_map])\n",
    "\n",
    "        #Exiting the loop \n",
    "        if(child[3][feature] == 1):\n",
    "            \n",
    "            mask = child[1][:, feature] > prev_idx\n",
    "            \n",
    "            X_child = np.delete(child[1][mask, :], feature, axis = 1)\n",
    "            y_child = child[2][mask]\n",
    "            X_type_child = np.delete(child[3], feature)\n",
    "            y_type_child = child[4]\n",
    "\n",
    "            new_child = Node()\n",
    "            childs.append(new_child)\n",
    "            q.append([new_child, X_child, y_child, X_type_child, y_type_child, child[5] + 1, feature_map])\n",
    "\n",
    "        return q, childs\n",
    "\n",
    "    def _select_best_feature(self, features, X, y, X_type, y_type):\n",
    "        \n",
    "        entropy_gain, variance_gain, Threshold = -np.inf, -np.inf, None\n",
    "        ent_best_feature, var_best_feature = None, None\n",
    "        curr_gain = None\n",
    "        flag = None\n",
    "        for feature in features:\n",
    "            if(X_type[feature] == 0):\n",
    "                #categorical\n",
    "                threshold = np.unique(X[:, feature])\n",
    "            else:\n",
    "                #continuous\n",
    "                threshold = self._find_threshold_continuous(feature, X, y, y_type)\n",
    "            \n",
    "            if(y_type):\n",
    "                #con\n",
    "                curr_gain = self._variance_gain(feature, X, y, threshold)\n",
    "                if(variance_gain < curr_gain):\n",
    "                    variance_gain = curr_gain\n",
    "                    var_best_feature = feature\n",
    "                    Threshold = threshold\n",
    "            else :\n",
    "                #cat\n",
    "                curr_gain = self._entropy_gain(feature, X, y, threshold)\n",
    "                if(entropy_gain < curr_gain):\n",
    "                    entropy_gain = curr_gain\n",
    "                    ent_best_feature = feature\n",
    "                    Threshold = threshold\n",
    "                \n",
    "        if ent_best_feature is not None:\n",
    "            return ent_best_feature, Threshold\n",
    "        else:\n",
    "            return var_best_feature, Threshold\n",
    "\n",
    "    def _entropy_gain(self, feature, X, y, threshold):\n",
    "        \n",
    "        parent_gain = self._entropy(y)\n",
    "        weighted_children = 0.0\n",
    "        for idx in threshold:\n",
    "            mask = X[:, feature] == idx\n",
    "            y_child = y[mask]\n",
    "            weighted_children = weighted_children + (len(y_child)/len(y)) * (self._entropy(y_child))\n",
    "        gain = parent_gain - weighted_children\n",
    "        \n",
    "        return gain\n",
    "    \n",
    "    def _variance_gain(self, feature, X, y, threshold):\n",
    "        parent_gain = self._variance(y)\n",
    "        weighted_children = 0.0\n",
    "        for idx in threshold:\n",
    "            mask = X[:, feature] < idx # Caution: Here i am pre assuming that the threshold is divided such that it can be used in intervels\n",
    "            y_child = np.array((y))[mask]\n",
    "            weighted_children = weighted_children + (len(y_child)/len(y)) * (self._variance(y_child))\n",
    "        gain = parent_gain - weighted_children\n",
    "        return gain\n",
    "        \n",
    "    def _variance(self, y):\n",
    "        return np.var(y) # Caution: Donot work for the other types than numericals\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        n = len(y)\n",
    "        if(n == 0):\n",
    "            return 0.0\n",
    "            \n",
    "        values, count = np.unique(y, return_counts = True)\n",
    "        entropy = 0.0\n",
    "        for i in count:\n",
    "            if(i != 0):\n",
    "                entropy = entropy + (i/n) * np.log2(i/n)\n",
    "        return -1 * entropy\n",
    "    \n",
    "    #Only for the continuous features\n",
    "    def _find_threshold_continuous(self, feature, X, y, y_type):\n",
    "        #if the label is continuos\n",
    "        n_childs = self.avg_childs\n",
    "        feature_map = np.stack((X[:, feature], y), axis = 1)\n",
    "        feature_map = feature_map[feature_map[:, 1].argsort()]\n",
    "\n",
    "        threshold = []\n",
    "        if y_type == 1:\n",
    "            #con label\n",
    "            n = int(len(y) / n_childs)\n",
    "            for i in range(n_childs):\n",
    "                start = n * i\n",
    "                end = n * (i + 1)\n",
    "                if end >= (len(y)):\n",
    "                    break\n",
    "                threshold.append(feature_map[start : end, 0].mean(axis = 0))\n",
    "        else:\n",
    "            #cat label\n",
    "            for i in range(len(feature_map) - 1):\n",
    "                if(feature_map[i, 1] != feature_map[i+1, 1]):\n",
    "                    threshold.append(feature_map[i, 0])\n",
    "        return threshold\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1262d20-67cd-4334-a886-f298e27c3fc5",
   "metadata": {},
   "source": [
    "## Implementing a Node class\n",
    "### This class has the node and the properties of the depth which is used to compare with the max  depth while making decision tree\n",
    "1. Creation of the Node (init)\n",
    "2. Value of the node\n",
    "3. Predicting the value of the Decision Tree is usaually done here\n",
    "4. Predicting is the way of traversing\n",
    "\n",
    "### **Note : We can use the DSA techniques like back tracking and other graph/ tree properties here to exploit and reverse engineer the tree** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4547499f-515a-4e63-9d53-bd5267246fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature = None, feature_type = None, dividers = None, childs = None, *, value = None):\n",
    "        self.feature = feature\n",
    "        self.feature_type = feature_type\n",
    "        self.dividers = dividers if dividers is not None else []\n",
    "        self.childs = childs if childs is not None else []\n",
    "        #Only for the leaf nodes\n",
    "        self.value = value\n",
    "\n",
    "    def depth(self):\n",
    "        if(len(self.childs) == 0):\n",
    "            return 0\n",
    "        return 1 + max([child.depth() for child in self.childs])\n",
    "\n",
    "    def predict(self, x):\n",
    "        # shape of x is (1, n)\n",
    "        if self.value is not None:\n",
    "            return self.value\n",
    "\n",
    "        if(self.feature_type == 0):\n",
    "            #categorical\n",
    "            idx_value = x[self.feature]\n",
    "            for i in range(len(self.dividers)):\n",
    "                if(self.dividers[i] == idx_value):\n",
    "                    return self.childs[i].predict(x)\n",
    "            # If it comes out without matching anything\n",
    "            print(\"Caution: The new value isn't in the data category so choosing a random child\")\n",
    "            i = np.random.randint(0, len(self.childs))\n",
    "            return self.childs[i].predict(x)\n",
    "        else:\n",
    "            #continuous\n",
    "            idx_value = x[self.feature]\n",
    "            for i in range(len(self.dividers)):\n",
    "                if(self.dividers[i] > idx_value):\n",
    "                    return self.childs[i].predict(x)\n",
    "            # This is valid because the childs are 1 more than the dividers\n",
    "            return self.childs[-1].predict(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bb6ac9-5cfc-45ca-b253-9bd4e61ff2fe",
   "metadata": {},
   "source": [
    "## ------- debugging --------\n",
    "1. I have used DFS in the forming of the tree which is not a standard way and the problem arised is that we cannot delete some feature data\n",
    "   so every feature is traversed in an infinite loop\n",
    "   Solution => Use BFS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fee6569-87ba-4eac-a1bc-e42e300be5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caution: The new value isn't in the data category so choosing a random child\n",
      "Caution: The new value isn't in the data category so choosing a random child\n",
      "Caution: The new value isn't in the data category so choosing a random child\n",
      "Caution: The new value isn't in the data category so choosing a random child\n",
      "38 42\n"
     ]
    }
   ],
   "source": [
    "parent = Node()\n",
    "d = Decision_Tree()\n",
    "d.build_tree(parent, X, y, X_type, y_type)\n",
    "X = np.array(([55, \"MALE\", \"LOW\", \"HIGH\", 12]), dtype = object)\n",
    "count = 0\n",
    "count_any = 0\n",
    "for i in range(50):\n",
    "    d = parent.predict(X_test[i])\n",
    "    y = max(d, key = d.get)\n",
    "    \n",
    "    if(y == y_test[i]):\n",
    "        count += 1\n",
    "    if(y_test[i] in d):\n",
    "        count_any += 1\n",
    "print(count, count_any)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
