{"cells":[{"cell_type":"markdown","metadata":{"id":"emjAQ_34SozP","tags":[]},"source":["# Programming Assignment 1"]},{"cell_type":"markdown","metadata":{"id":"tKkKCHwsSozQ"},"source":["In Part 1, you have to implement a linear regression model to predict the price of a house based on various input features.\n","\n","You have to write your code in this jupyter notebook and submit the solved jupyter notebook with the file name \\<Roll_No\\>_A1_1.ipynb for evaluation. You have to enter your code only in those cells which are marked as ```## CODE REQUIRED ##```, and you have to write your code only between ```### START CODE HERE ###``` and ```### END CODE HERE ###``` comments."]},{"cell_type":"markdown","metadata":{"id":"jHPf8pHpSozR"},"source":["## Part 1: Linear Regression\n","\n","### Problem Statement  \n","A real estate company is building a machine learning model to determine the price of a house. The model will take various information regarding a house as input features and predict the price per unit area. They decided to use the linear regression as the machine learning model. Your task is to help the company to build the model.\n","Given various features of a house, you will create a linear regression model to predict the price of the house.\n","\n","### Data Description\n","\n","Dataset Filename: `housing_dataset.csv`\n","\n","Attributes Information:\n","1. CRIM: per capita crime rate by town\n","2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n","3. INDUS: proportion of non-­retail business acres per town\n","4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n","5. NOX: nitric oxides concentration (parts per 10 million)\n","6. RM: average number of rooms per dwelling\n","7. AGE: proportion of owner­occupied units built prior to 1940\n","8. DIS: weighted distances to five Boston employment centres\n","9. RAD: index of accessibility to radial highways\n","10. TAX: full-­value property-­tax rate per $10,000\n","\n","11. PTRATIO: pupil-­teacher ratio by town\n","12. B: 1000(Bk ­- 0.63)^2 where Bk is the proportion of blacks by town\n","13. LSTAT: % lower status of the population\n","\n","Target Variable: MEDV: Median value of owner-­occupied homes in $1000's\n","\n","\n","These are the following steps or functions that you have to complete to create and train the linear regression model:\n","1. Reading the data\n","2. Computing the loss function\n","3. Computing the gradient of the loss\n","4. Training the model using Batch Gradient Descent\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rJDNL8y8SozR","tags":[]},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import copy\n","import math\n","import random"]},{"cell_type":"markdown","metadata":{"id":"S5ExYIXiSozS"},"source":["### 1.1. Reading the data"]},{"cell_type":"markdown","metadata":{"id":"FgnxuEDkSozS"},"source":["In the following function ```load_data```, you have to read the data from the file and store the data into a pandas dataframe. Then you have to create two numpy arrays $X$ and $y$ from the dataframe:\n","\n","+ $X$: Input data of the shape (number of samples, number of input features)\n","+ $y$: Target variable of the shape (number of samples,)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DEal5SFmSozS","tags":[]},"outputs":[],"source":["## CODE REQUIRED ##\n","\n","def load_data(filepath):\n","    \"\"\"\n","    This function loads the data into a pandas dataframe and coverts it into X and y numpy arrays\n","\n","    Args:\n","        filepath: File path as a string\n","    Returns:\n","        X: Input data of the shape (# of samples, # of input features)\n","        y: Target variable of the shape (# of sample,)\n","    \"\"\"\n","\n","    ### START CODE HERE ###\n","\n","    ### END CODE HERE ###\n","\n","    return X,y\n","\n","filepath = None\n","### START CODE HERE ###\n","## set the file path\n","\n","### END CODE HERE ###\n","X, y = load_data(filepath)\n","\n","print(\"Shape of X: \",X.shape, \"Shape of y: \",y.shape)"]},{"cell_type":"markdown","metadata":{"id":"4FAc8UixtqLS"},"source":["We need to pre-process the data. We are using min-max scaler to scale the input data ($X$).\n","\n","After that, we split the data (```X``` and ```y```) into a training dataset (```X_train``` and ```y_train```) and test dataset (```X_test``` and ```y_test```)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LLmD1I3-SozT","tags":[]},"outputs":[],"source":["## Data scaling and train-test split\n","\n","def train_test_split(X, y, test_size=0.25, random_state=None):\n","    if random_state is not None:\n","        np.random.seed(random_state)\n","    indices = np.arange(X.shape[0])\n","    np.random.shuffle(indices)\n","\n","    split_index = int(X.shape[0] * (1 - test_size))\n","\n","    train_indices = indices[:split_index]\n","    test_indices = indices[split_index:]\n","\n","    X_train = X[train_indices]\n","    X_test = X[test_indices]\n","    y_train = y[train_indices]\n","    y_test = y[test_indices]\n","\n","    return X_train, X_test, y_train, y_test\n","\n","def min_max_scaler(X, feature_range=(0, 1)):\n","    X_min = np.min(X, axis=0)\n","    X_max = np.max(X, axis=0)\n","\n","    X_scaled = (X-X_min)/(X_max-X_min)\n","\n","    return X_scaled\n","\n","\n","X = min_max_scaler(X)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n","print(\"Shape of X_train: \",X_train.shape, \"Shape of y_train: \",y_train.shape)\n","print(\"Shape of X_test: \",X_test.shape, \"Shape of y_test: \",y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"xrFEUV2-SozU"},"source":["### 1.2. Computing the Loss Function"]},{"cell_type":"markdown","metadata":{"id":"I1BC0TPCSozU"},"source":["In linear regression, the model parameters are:\n","\n","+ $w$: Parameters of the linear regression model (excluding the bias) of the shape (1, number of features)\n","\n","+ $b$: Bias parameter (scalar) of the linear regression model\n","\n","Both $w$ and $b$ are numpy arrays.\n","\n","Given the model parameters $w$ and $b$, the prediction for an input sample $X^i$ is:\n","$$h_{w,b}(X^i) = w \\cdot X^i + b$$\n","where $X^i$ is the $i^{th}$ training sample with shape (number of features,1)\n","\n","For linear regression, you have to implement and compute Mean Squarred Error loss fucntion:\n","$$ L_{w,b}(X) = \\sum_{i=1}^{m}(y^i - h_{w,b}(X^i))^2 $$\n","where $y^i$ is the true target value for the $i^{th}$ sample and $h_{w,b}(X^i)$ is the predicted value for the $i^{th}$ sample using the parameters $w$ and $b$.\n","\n","$w$ is the list of parameters excluding the bias and $b$ is the bias term."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t6GqZMlYSozV","tags":[]},"outputs":[],"source":["## CODE REQUIRED ##\n","\n","def loss_function(X, y, w, b):\n","    \"\"\"\n","    Computes the cost function for linear regression.\n","\n","    Args:\n","        X: Input data of the shape (# of training samples, # of input features)\n","        y: Target variable of the shape (# of training sample,)\n","        w: Parameters of the linear regression model (excluding the bias) of the shape (1, number of features)\n","        b: Bias parameter (scalar) of the linear regression model\n","\n","    Returns\n","        loss: The loss function value of using w and b as the parameters to fit the data points in X and y\n","    \"\"\"\n","    # number of training examples\n","    m = X.shape[0]\n","\n","    ### START CODE HERE ###\n","\n","\n","    ### END CODE HERE ###\n","\n","    return loss"]},{"cell_type":"markdown","metadata":{"id":"l0h7jhxGSozV"},"source":["### 1.3. Comptuing the Gradient of the Loss\n","\n","In this following function ```compute_gradient```, you have to compute the gradients $\\frac{\\partial L}{\\partial w}$ and $\\frac{\\partial L}{\\partial b}$ of the loss $L$ w.r.t. $w$ and $b$. More specifically, you have to iterate over every training example and compute the gradients of the loss for that training example. Finally, aggregate the gradient values for all the training examples and take the average. The gradients can be computed as:\n","$$\\frac{\\partial L}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^m (h_{w,b}(X^i)-y^i)X^i$$\n","\n","$$\\frac{\\partial L}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (h_{w,b}(X^i)-y^i)$$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"do-IiCTZSozV","tags":[]},"outputs":[],"source":["## CODE REQUIRED ##\n","\n","def compute_gradient(X, y, w, b):\n","    \"\"\"\n","    Computes the gradient values\n","    Args:\n","       X: Input data of the shape (# of training samples, # of input features)\n","       y: Target variable of the shape (# of training sample,)\n","       w: Parameters of the linear regression model (excluding the bias) of the shape (1, number of features)\n","       b: Bias parameter of the linear regression model of the shape (1,1) or a scaler\n","    Returns:\n","       dL_dw : The gradient of the cost w.r.t. the parameters w with shape same as w\n","       dL_db : The gradient of the cost w.r.t. the parameter b with shape same as b\n","    \"\"\"\n","\n","    # Number of training examples\n","    m = X.shape[0]\n","\n","    dL_dw = None\n","    dL_db = None\n","\n","    ### START CODE HERE ###\n","\n","    ### END CODE HERE ###\n","    return dL_dw, dL_db"]},{"cell_type":"markdown","metadata":{"id":"uYL1jWaZSozV"},"source":["### 1.4. Training the Model using Batch Gradient Descent"]},{"cell_type":"markdown","metadata":{"id":"2vwyRzWqSozV"},"source":["Finally, you have to implement the batch gradient descent algorithm to train and learn the parameters of the linear regression model. You have to use ```loss_function``` and ```compute_gradient``` functions that you have implemented earlier in this assignment.\n","\n","In this ```batch_gradient_descent``` function, you have to compute the gradient for the training samples and update the parameters $w$ and $b$ in every iteration:\n","\n","+ $w \\leftarrow w - \\alpha \\frac{\\partial L}{\\partial w}$\n","\n","+ $b \\leftarrow b - \\alpha \\frac{\\partial L}{\\partial b}$\n","\n","Additionally, you have compute the loss function values in every iteration and store it in the list variable ```loss_hist``` and print the loss value after every 100 iterations during the training process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UHKldb3hSozV","tags":[]},"outputs":[],"source":["## CODE REQUIRED ##\n","\n","def batch_gradient_descent(X, y, w_initial, b_initial, alpha, num_iters):\n","    \"\"\"\n","    Batch gradient descent to learn the parameters (w and b) of the linear regression model and to print loss values\n","    every 100 iterations\n","\n","    Args:\n","        X: Input data of the shape (# of training samples, # of input features)\n","        y: Target variable of the shape (# of training sample,)\n","        w_initial: Initial parameters of the linear regression model (excluding the bias) of the shape (1, number of features)\n","        b_initial: Initial bias parameter (scalar) of the linear regression model\n","        alpha: Learning rate\n","        num_iters: number of iterations\n","    Returns\n","        w: Updated values of parameters of the model after training\n","        b: Updated bias of the model after training\n","        loss_hist: List of loss values for every iteration\n","    \"\"\"\n","\n","    # number of training examples\n","    m = X.shape[0]\n","\n","    # to store loss values for every iteation as a list and print loss value after every 100 iterations\n","    loss_hist = []\n","\n","    # Initialize parameters\n","    w = copy.deepcopy(w_initial) ## deepcopy is used so that the updates do not change the initial variable values\n","    b = b_initial\n","\n","    ### START CODE HERE ###\n","\n","\n","\n","    ### END CODE HERE ###\n","\n","    return w, b, loss_hist"]},{"cell_type":"markdown","metadata":{"id":"y0h3DUMwNocp"},"source":["Now you have to intialize the model parameters ($w$ and $b$) and learning rate (```alpha```). The learning rate ```alpha``` is to be initialized as 0.001."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xd8QFReNDyGs"},"outputs":[],"source":["## CODE REQUIRED ##\n","\n","def initialize_parameters():\n","    \"\"\"\n","    This function randomly initializes the model parameters (w and b) and the hyperparameter alpha\n","    Initial w and b should be randomly sampled from a normal distribution with mean 0\n","    alpha should be initialized as 0.001\n","    Args:\n","        None\n","    Returns:\n","        initial_w: Initial parameters of the linear regression model (excluding the bias) of the shape (1, number of features)\n","        initial_b: Initial bias parameter (scalar) of the linear regression model\n","        alpha: Learning rate\n","    \"\"\"\n","\n","    initial_w = None\n","    initial_b = None\n","    alpha = None\n","\n","    ### START CODE HERE ###\n","\n","\n","\n","    ### END CODE HERE ###\n","\n","    return initial_w,initial_b,alpha\n"]},{"cell_type":"markdown","metadata":{"id":"l7nt5VdORUEu"},"source":["In the next cell, the model is trained using batch gradient descent algorithm for ```num_iters=10000``` iterations. You can change the number of iterations to check any improvements in the performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9rAYubtASozV","tags":[]},"outputs":[],"source":["# initialize the parameters and hyperparameter\n","initial_w, initial_b, alpha = initialize_parameters()\n","\n","# number of iterations\n","num_iters = 10000\n","\n","w,b,loss_hist = batch_gradient_descent(X_train ,y_train, initial_w, initial_b, alpha, num_iters)\n","print(\"Updated w: \",w)\n","print(\"Updated b: \",b)"]},{"cell_type":"markdown","metadata":{"id":"sluek2LkSozW"},"source":["### 1.5. Final Train Error and Test Error"]},{"cell_type":"markdown","metadata":{"id":"EH9oHC10fAlV"},"source":["After the linear regression model is trained, we will compute the final train error and test error for the trained model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UIXb7sSvSozW","tags":[]},"outputs":[],"source":["## Train and Test error computation\n","\n","train_error = loss_function(X_train,y_train,w,b)\n","test_error = loss_function(X_test,y_test,w,b)\n","print(\"Train Error: \",train_error, \", Test Error: \",test_error)"]},{"cell_type":"markdown","metadata":{"id":"eximd12PbAgC"},"source":["### 1.6. Plotting the loss function"]},{"cell_type":"markdown","metadata":{"id":"meDwLCCIfOBo"},"source":["We will plot the loss function values for every training iteration. If the model is trained properly, you will see that the loss function reduces as the training progesses and it converges at some point."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DW4bue0oSozW"},"outputs":[],"source":["# PLotting the loss values for every training iterations\n","\n","loss_plot = [loss_hist[i][0][0] for i in range(len(loss_hist))]\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"Loss function\")\n","plt.plot(loss_plot)\n","plt.show()"]},{"cell_type":"markdown","source":["### 1.7. Plotting the histogram of loss"],"metadata":{"id":"ymvI0qD3l3HT"}},{"cell_type":"code","source":["def mse_losses(X, y, w, b):\n","    y_pred = np.dot(X, w.T) + b\n","    losses = (y_pred - y)**2\n","    return losses"],"metadata":{"id":"ed3uwMuE0POa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Compute squared error loss for linear regression\n","loss_linear = mse_losses(X_test,y_test,w,b)\n","\n","# Plot histograms\n","plt.figure(figsize=(12, 6))\n","\n","# Linear regression loss histogram\n","plt.subplot(1, 2, 1)\n","plt.hist(loss_linear, bins=20, color='blue', alpha=0.7, edgecolor='black')\n","plt.title('Loss Histogram: Linear Regression')\n","plt.xlabel('Squared Error Loss')\n","plt.ylabel('Frequency')\n","\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"CyGr8hffkllW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BAVgq4HWCmDX"},"source":["### 1.8 Experimenting with different values of the Hyperparemeters\n","\n","Previously, we have took the learning rate as 0.001. Now, you have to train the model again by taking learning rate as-\n","\n","\n","1.   0.01\n","2.   0.001\n","\n","\n","After the model is trained, you have to compare the performance of the model with these chosen hyperparameters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K2QuEbE1Clxq"},"outputs":[],"source":["## CODE REQUIRED ##\n","\n","\"\"\"\n","Compare the performance with the chosen hyperparameters.\n","\"\"\"\n","\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"S34KA2LlXXbR"},"source":["## Task: Submitting Your Predictions\n","\n","After training your Linear Regression model on the provided training dataset, you have generate predictions for the test dataset named `housing_test.csv`, your final task is to save the predicted values for the test dataset in a file named `RollNo_Linear.csv`.\n","\n","### Instructions for Submission:\n","1. **Format**:  \n","   The `RollNo_Linear.csv` file should contain one prediction per line, corresponding to the order of the test dataset features provided to you. Ensure there are no extra spaces, commas, or blank lines.\n","\n","2. **File Name**:  \n","The file must be named `RollNo_Linear.csv` exactly (case-sensitive).\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y0I1xsTIXXbR"},"outputs":[],"source":["\"\"\"\n","Write the code to save the predictions in required format\n","\"\"\"\n","### START CODE HERE ###\n","\n","### END CODE HERE ###"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}